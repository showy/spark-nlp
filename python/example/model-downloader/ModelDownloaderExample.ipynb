{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary imports, start spark and create our model downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "print(sys.version)\n",
    "\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"downloader-example\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# instantiate the downloader\n",
    "downloader = ResourceDownloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dummy spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some mock data to play with\n",
    "l = [\n",
    "  (1,'To be or not to be'),\n",
    "  (2,'This is it!')\n",
    "]\n",
    "\n",
    "data = spark.createDataFrame(l, ['docID','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we intend to download a POS model by its name and language, which requires tokenized text. Hence, we create our tokenizer pipeline to get the data ready.\n",
    "Then, we add the POS along the other annotators and transform some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download directly - models\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "    \n",
    "# pos tagger\n",
    "pos = downloader.downloadModel(PerceptronModel, \"pos_fast\", \"en\")    \n",
    "    \n",
    "pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos])\n",
    "\n",
    "output = pipeline.fit(data).transform(data)\n",
    "output.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download a Pipeline by its name and language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download directly - pipeline models\n",
    "\n",
    "# simple pipeline with document assembler and tokenizer\n",
    "pipeline = downloader.downloadPipeline(\"pipeline_basic\", \"en\")\n",
    "pipeline.transform(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clear cache of recently downloaded pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test clearCache\n",
    "downloader.clearCache(\"pipeline_basic\", \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use predefined BasicPipeline in order to annotate a dataframe with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download predefined - pipelines\n",
    "from sparknlp.pretrained.pipeline.en import BasicPipeline\n",
    "\n",
    "basic_data = BasicPipeline.annotate(data, \"text\")\n",
    "basic_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also annotate a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotat quickly from string\n",
    "BasicPipeline().annotate(\"This world is made up of good and bad things\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proceed to download a POS model, utilizing the PerceptronModel class to retrieve it.\n",
    "We do the same for the NER model.\n",
    "Then, we retrieve the Basic Pipeline and combine these models to use them appropriately meeting their requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download predefined - models\n",
    "\n",
    "pos = PerceptronModel.pretrained()\n",
    "pos.setInputCols([\"document\", \"normal\"]).setOutputCol(\"pos\")\n",
    "\n",
    "ner = NerCrfModel.pretrained()\n",
    "ner.setInputCols([\"pos\", \"normal\", \"document\"]).setOutputCol(\"ner\")\n",
    "\n",
    "annotation_pipeline = BasicPipeline.pretrained()\n",
    "annotation_data = annotation_pipeline.transform(data)\n",
    "annotation_data.show()\n",
    "\n",
    "pos_tagged = pos.transform(annotation_data)\n",
    "ner_tagged = ner.transform(pos_tagged)\n",
    "ner_tagged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
