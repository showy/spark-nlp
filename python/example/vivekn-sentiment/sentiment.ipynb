{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../')\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+\n",
      "|itemid|sentiment|                text|\n",
      "+------+---------+--------------------+\n",
      "|     1|        0|                 ...|\n",
      "|     2|        0|                 ...|\n",
      "|     3|        1|              omg...|\n",
      "|     4|        0|          .. Omga...|\n",
      "|     5|        0|         i think ...|\n",
      "|     6|        0|         or i jus...|\n",
      "|     7|        1|       Juuuuuuuuu...|\n",
      "|     8|        0|       Sunny Agai...|\n",
      "|     9|        1|      handed in m...|\n",
      "|    10|        1|      hmmmm.... i...|\n",
      "|    11|        0|      I must thin...|\n",
      "|    12|        1|      thanks to a...|\n",
      "|    13|        0|      this weeken...|\n",
      "|    14|        0|     jb isnt show...|\n",
      "|    15|        0|     ok thats it ...|\n",
      "|    16|        0|    &lt;-------- ...|\n",
      "|    17|        0|    awhhe man.......|\n",
      "|    18|        1|    Feeling stran...|\n",
      "|    19|        0|    HUGE roll of ...|\n",
      "|    20|        0|    I just cut my...|\n",
      "+------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the input data to be annotated\n",
    "data = spark. \\\n",
    "        read. \\\n",
    "        parquet(\"file:///\" + os.getcwd() + \"/../../../src/test/resources/sentiment.parquet\"). \\\n",
    "        limit(1000)\n",
    "data.cache()\n",
    "data.count()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the dataframe\n",
    "document_assembler = DocumentAssembler() \\\n",
    "            .setInputCol(\"text\")\n",
    "        \n",
    "### Transform input to appropriate schema\n",
    "#assembled = document_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sentence detector\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "#sentence_data = sentence_detector.transform(checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer\n",
    "tokenizer = Tokenizer() \\\n",
    "            .setInputCols([\"sentence\"]) \\\n",
    "            .setOutputCol(\"token\")\n",
    "#tokenized = tokenizer.transform(assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer() \\\n",
    "            .setInputCols([\"token\"]) \\\n",
    "            .setOutputCol(\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spell Checker\n",
    "spell_checker = NorvigSweetingApproach() \\\n",
    "            .setInputCols([\"normal\"]) \\\n",
    "            .setOutputCol(\"spell\")\n",
    "\n",
    "#checked = spell_checker.fit(tokenized).transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_detector = ViveknSentimentApproach() \\\n",
    "    .setInputCols([\"spell\", \"sentence\"]) \\\n",
    "    .setOutputCol(\"sentiment\") \\\n",
    "    .setPruneCorpus(0) \\\n",
    "    .setPositiveSource(\"file:///\" + os.getcwd() + \"/../../../src/test/resources/vivekn/positive\") \\\n",
    "    .setNegativeSource(\"file:///\" + os.getcwd() + \"/../../../src/test/resources/vivekn/negative\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = PerceptronApproach() \\\n",
    "    .setInputCols([\"sentence\", \"spell\"]) \\\n",
    "    .setOutputCol(\"pos\") \\\n",
    "    .setCorpus(\"file:///\" + os.getcwd() + \"/../../../src/test/resources/anc-pos-corpus-small/\", delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"sentiment\"]) \\\n",
    "    .setIncludeKeys(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|         hello world|\n",
      "|this is some more...|\n",
      "|and here another ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.sparkContext.parallelize([[\"hello world\"], [\"this is some more text\"], [\"and here another sentence\"]]).toDF().toDF(\"text\")\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|itemid|sentiment|                text|            document|            sentence|               token|              normal|               spell|\n",
      "+------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     1|        0|                 ...|[[document, 0, 60...|[[document, 21, 4...|[[token, 21, 22, ...|[[token, 21, 22, ...|[[token, 21, 22, ...|\n",
      "|     2|        0|                 ...|[[document, 0, 50...|[[document, 19, 4...|[[token, 19, 19, ...|[[token, 19, 19, ...|[[token, 19, 19, ...|\n",
      "|     3|        1|              omg...|[[document, 0, 36...|[[document, 14, 3...|[[token, 14, 16, ...|[[token, 14, 16, ...|[[token, 14, 16, ...|\n",
      "|     4|        0|          .. Omga...|[[document, 0, 13...|[[document, 10, 1...|[[token, 10, 10, ...|[[token, 13, 18, ...|[[token, 13, 18, ...|\n",
      "|     5|        0|         i think ...|[[document, 0, 52...|[[document, 9, 42...|[[token, 9, 9, i,...|[[token, 9, 9, i,...|[[token, 9, 9, i,...|\n",
      "|     6|        0|         or i jus...|[[document, 0, 41...|[[document, 9, 33...|[[token, 9, 10, o...|[[token, 9, 10, o...|[[token, 9, 10, o...|\n",
      "|     7|        1|       Juuuuuuuuu...|[[document, 0, 40...|[[document, 7, 40...|[[token, 7, 30, J...|[[token, 7, 30, j...|[[token, 7, 30, j...|\n",
      "|     8|        0|       Sunny Agai...|[[document, 0, 60...|[[document, 7, 60...|[[token, 7, 11, S...|[[token, 7, 11, s...|[[token, 7, 11, s...|\n",
      "|     9|        1|      handed in m...|[[document, 0, 52...|[[document, 6, 33...|[[token, 6, 11, h...|[[token, 6, 11, h...|[[token, 6, 11, h...|\n",
      "|    10|        1|      hmmmm.... i...|[[document, 0, 45...|[[document, 6, 11...|[[token, 6, 10, h...|[[token, 6, 10, h...|[[token, 6, 10, h...|\n",
      "|    11|        0|      I must thin...|[[document, 0, 34...|[[document, 6, 33...|[[token, 6, 6, I,...|[[token, 6, 6, i,...|[[token, 6, 6, i,...|\n",
      "|    12|        1|      thanks to a...|[[document, 0, 60...|[[document, 6, 52...|[[token, 6, 11, t...|[[token, 6, 11, t...|[[token, 6, 11, t...|\n",
      "|    13|        0|      this weeken...|[[document, 0, 35...|[[document, 6, 35...|[[token, 6, 9, th...|[[token, 6, 9, th...|[[token, 6, 9, th...|\n",
      "|    14|        0|     jb isnt show...|[[document, 0, 42...|[[document, 5, 42...|[[token, 5, 6, jb...|[[token, 5, 6, jb...|[[token, 5, 6, jb...|\n",
      "|    15|        0|     ok thats it ...|[[document, 0, 24...|[[document, 5, 24...|[[token, 5, 6, ok...|[[token, 5, 6, ok...|[[token, 5, 6, ok...|\n",
      "|    16|        0|    &lt;-------- ...|[[document, 0, 51...|[[document, 4, 7,...|[[token, 4, 4, &,...|[[token, 5, 6, lt...|[[token, 5, 6, lt...|\n",
      "|    17|        0|    awhhe man.......|[[document, 0, 10...|[[document, 4, 15...|[[token, 4, 8, aw...|[[token, 4, 8, aw...|[[token, 4, 8, yw...|\n",
      "|    18|        1|    Feeling stran...|[[document, 0, 81...|[[document, 4, 26...|[[token, 4, 10, F...|[[token, 4, 10, f...|[[token, 4, 10, f...|\n",
      "|    19|        0|    HUGE roll of ...|[[document, 0, 47...|[[document, 4, 33...|[[token, 4, 7, HU...|[[token, 4, 7, hu...|[[token, 4, 7, hu...|\n",
      "|    20|        0|    I just cut my...|[[document, 0, 13...|[[document, 4, 27...|[[token, 4, 4, I,...|[[token, 4, 4, i,...|[[token, 4, 4, i,...|\n",
      "+------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time elapsed pipeline process: 12.116255283355713\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker\n",
    "])\n",
    "\n",
    "start = time.time()\n",
    "sentiment_data = pipeline.fit(training).transform(data)\n",
    "sentiment_data.show()\n",
    "end = time.time()\n",
    "print(\"Time elapsed pipeline process: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in sentiment_data.take(5):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pipeline.write().overwrite().save(\"./ps\")\n",
    "pipeline.fit(data).write().overwrite().save(\"./ms\")\n",
    "end = time.time()\n",
    "print(\"Time elapsed in write pipelines: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "p = Pipeline.read().load(\"./ps\")\n",
    "pm = PipelineModel.read().load(\"./ms\")\n",
    "end = time.time()\n",
    "print(\"Time elapsed in read pipelines: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pm.transform(data).where(\"finished_sentiment not like '%negative%'\").show()\n",
    "print(pm.transform(data).count())\n",
    "end = time.time()\n",
    "print(\"Time elapsed in using loaded pipelines: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
